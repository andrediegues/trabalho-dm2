---
title: "Road Safety in the United Kingdom in 2015"
author: "André Diegues, André Fonseca, Rui Andrade"
date: "17 de Março de 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r prepare_data, echo=FALSE, message=FALSE}
library(arules)
library(arulesViz)
library(gdata)
library(dplyr)

get_period <- function(x) {
  sapply(x, function(x) {
    if ((x >= 800 & x < 900) | (x >= 1700 & x < 1900)) {
      "rush hour"
    }
    else if (x >= 900 & x < 1200) {
      "morning"
    }
    else if (x >= 1200 & x < 1700) {
      "afternoon"
    }
    else {
      "night"
    }
  }
  )
}

police_forceInterval <- function(x) {
  sapply(x, function(x) {
    if (x >= 1 & x < 6) {
      "small"
    }
    else {
      if (x >= 6 & x < 45) {
        "medium"
      }
      else {
        "large"
      }
    }
  })
}

vehicle_And_Casualty_Interval <- function(x) {
  sapply(x, function(x) {
    if (x == 1) {
      "individual"
    }
    else {
      if (x > 1) {
        "multiple"
      }
      else {
        "none"
      }
    }
  })
}

######################################READ CSV#########################################

data <- read.csv("Accidents_2015.csv", header=TRUE)

######################################READ THE GUIDES###################################

file_guide <-"Road-Accident-Safety-Data-Guide.xls"
read_guides <- function(file_from,sheet_name){
  data_name <-read.xls(file_from,sheet =sheet_name)
  return(data_name)
}

##Read Casulty Severity
Casualty_Severity <-read_guides(file_guide,"Casualty Severity")

## Read Ped Location
Ped_Location <- read_guides(file_guide,"Ped Location")

## Read Ped Movement
Ped_Movement <- read_guides(file_guide,"Ped Movement")

## Read Car Passenger 
Car_Passenger <- read_guides(file_guide,"Car Passenger")

## Read Bus Passenger
Bus_Passenger <- read_guides(file_guide,"Bus Passenger")

##Read Ped Road Maintenance Worker
Ped_Worker <- read_guides(file_guide,"Ped Road Maintenance Worker")

##Read Casualty Type
Casualty_Type <- read_guides(file_guide,"Casualty Type")

##Read IMD Decile
IMD_Decile <- read_guides(file_guide,"IMD Decile")

##Read Home Area Type
Home_Area <- read_guides(file_guide,"Home Area Type")

######################## CLEAN DATA ###############################
library(lubridate)


# Temos outra coluna para localizacao: LSOA_Of_Accident_Location
data$Location_Easting_OSGR = NULL 
data$Location_Northing_OSGR = NULL
# Nao e relevante:
data$Did_Police_Officer_Attend_Scene_of_Accident = NULL
data$Local_Authority_.Highway. = NULL
data$Local_Authority_.District. = NULL
# Tipos correctos:
data$Longitude <- as.double(data$Longitude)
data$Latitude <- as.double(data$Latitude)
data$Accident_Severity <- as.factor(data$Accident_Severity)
data$Weather_Conditions <- as.factor(data$Weather_Conditions)
# Separar datas por dia/mes/ano:
data$Date <- as.Date(data$Date, format="%d/%m/%Y")
dates <- data$Date
parsedDates <- data.frame(Date = dates, Day=lubridate::day(dates), 
                          Month=lubridate::month(dates, label=TRUE), 
                          Year=lubridate::year(dates), WeekDay=lubridate::wday(dates, label=TRUE))
data <- data.frame(data, parsedDates)
data["Date.1"] <- NULL
# Colocar Time em formato numerico, sem :
data$Time <- as.numeric(sub(":", "", data$Time))
# Remover linhas com NA
data <- data[complete.cases(data),]
# Organizar horas por periodo
data <- mutate(data, Period = get_period(Time))
data$Time <- as.factor(data$Period)
# Discretizar numero de policias
data <- mutate(data, PoliceForce = police_forceInterval(Police_Force))
data$PoliceForce <- as.factor(data$PoliceForce)
# Discretizar numero de veiculos 
data <- mutate(data, Number_Vehicles = vehicle_And_Casualty_Interval(Number_of_Vehicles))
data$Period <- as.factor(data$Number_Vehicles)
# Discretizar numero de casualidades
data <- mutate(data, Number_Casualties = vehicle_And_Casualty_Interval(Number_of_Casualties))
data$Number_Casualties <- as.factor(data$Number_Casualties)
```



## Introduction

Association rule mining is a concept that fits under descriptive tasks in the area of _Data Mining_. Originally used in what is called _Market Basket Analysis_, so as to analyse customer purchases and try to find unexpected associations between items, it has been generalised to help explain, for example, what factors could contribute towards a given characteristic in an observation. Sets of items that are commonly found together are called _frequent itemsets_ and relationships between items - in the sense that having certain items could favour having other certain items - are called _association rules_.

In this report, we study a dataset that contains several details on car accidents in the United Kingdom. This dataset was made available by the Government of the United Kingdom and dates back to 2015. Examples of information contained in it include: GPS location of the occurence, severity of accident, number of vehicles and casualties involved, time and date, type of road, speed limit, natural conditions (weather or light), road conditions and infrastructure available in the road (how are junctions handled, are there crossings available or not...). The goal of this work is to find frequent itemsets relating to the details of accidents, or in other words, what do accidents typically have related to one another, as well as finding association rules on cause and effect of traffic accidents.

```{r cars}
summary(cars)
```

## Preprocessing
We decided to do preprocessing of the data, in order to help find interesting relations using association rules. We started by creating a function that allowed us to read the xls file guides for our main dataset and consequently the xls file with the main dataset.
This allowed us to exclude the second and third columns and also the thirty-first column, as all the variables relative to local authority, since they were not correlated with the rest of the data.
We also verified that the dataset had missing values in some observations and therefore we excluded these observations from the data.
In a first analysis of the dataset, we realized that for a better perception of the data, we should discretize some numeric variables, only to improve our goal to find association rules on the cause and effect of traffic accidents. After this, we decided to do some exploratory analysis on the data, in order to check for potential correlations or for variables that are not necessary.


```{r prepare_graphs,  echo=FALSE, message=FALSE}

# Fazer corresponder ints a strings
data$Accident_Severity = Casualty_Severity[data$Accident_Severity,]$label

Weather_Conditions <- c()
Weather_Conditions[1] = "Fine, no high winds"
Weather_Conditions[2] = "Raining, no high winds"
Weather_Conditions[3] = "Snowing, no high winds"
Weather_Conditions[4] = "Fine + high winds"
Weather_Conditions[5] = "Raining + high winds"
Weather_Conditions[6] = "Snowing + high winds"
Weather_Conditions[7] = "Fog or mist"
Weather_Conditions[8] = "Other"
Weather_Conditions[9] = "Unknown"
data$Weather_Conditions = Weather_Conditions[data$Weather_Conditions]
data$Weather_Conditions = as.factor(data$Weather_Conditions)

FirstRdClass <- c()
FirstRdClass[1] = "Motorway"
FirstRdClass[2] = "A(M)"
FirstRdClass[3] = "A"
FirstRdClass[4] = "B"
FirstRdClass[5] = "C"
FirstRdClass[6] = "Unclassified"
data$X1st_Road_Class = FirstRdClass[data$X1st_Road_Class]
data$X1st_Road_Class = as.factor(data$X1st_Road_Class)

FirstPointImpact <- c()
FirstPointImpact[1] = "No impact"
FirstPointImpact[2] = "Front"
FirstPointImpact[3] = "Back"
FirstPointImpact[4] = "Offside"
FirstPointImpact[5] = "Nearside"
```

## Exploratory Analysis
In this part, we started by matching some of the code guides with our actual data for better interpretation.
After that, we generated some graphs with a certain interest to visualize some correlation.
For that purpose, our first plot, as we can see in _fig.1_, tries to find an association between accident locations and the severity of the accident.

```{r pressure, echo=FALSE, message=FALSE}
library(ggplot2)

# scatterplot of accident locations, color coded by level of severity
ggplot(data, aes(x=Longitude, y=Latitude, colour=Accident_Severity)) + 
  geom_point() + 
  ggtitle('Accident locations and level of severity (fig. 1)')
```

As we can see, we have scattered the accident locations per level of severity and found that the geographical dispersion is similar to the UK geographical terrain and the most frequent cases of Fatal accident are in the north of the country. Looking over these points, it is quite reasonable to assume that fatal accidents are directly related to the worst atmospheric conditions that are felt normally with more intensity in the north of the country (especially during winter).

Now, our goal was to relate to each type of accident severity by month, and know, in this way, the distribution of accidents per month as we can see in _fig.2_.

```{r pressure2, echo=FALSE}
# severity of accidents by month
data_graph2 <- dplyr::select(data, c(Month, Accident_Severity))
data_graph2 <- dplyr::group_by(data_graph2, Accident_Severity)
data_graph2 <- dplyr::count(data_graph2, Month, Accident_Severity)
ggplot(data_graph2, aes(y=n, x=Accident_Severity)) + 
  geom_bar(stat="identity") + 
  ggtitle('Severity of accidents by month (fig. 2)') + facet_wrap(~ Month)
```

Observing _fig.2_, we can clearly see that we have a decreasing distribution of accident from Slight to Fatal almost identical on all year.
Observing the data in a more detailed way, we observe that the month of December is the most deadly and in some way supports our claims on _fig.1_.

Now, our goal was to find out how the Accident Severity would behave when compared to the atmospheric conditions.
From this data, we expect to see a higher frequency of Fatal accidents when put against bad weather conditions, as we can see on _fig.3_.

```{r pressure3, echo=FALSE}
# severity of accidents by weather conditions
data_graph3 <- dplyr::select(data, c(Weather_Conditions, Accident_Severity))
data_graph3 <- dplyr::group_by(data_graph3, Weather_Conditions, Accident_Severity)
data_graph3 <- dplyr::count(data_graph3, Weather_Conditions, Accident_Severity)
data_graph3 <- dplyr::mutate(data_graph3, fr=n/sum(n))
ggplot(data_graph3, aes(y=fr, x=Accident_Severity)) + 
  geom_bar(stat="identity") + 
  ggtitle('Severity of accidents by weather conditions (fig. 3)') + facet_wrap(~ Weather_Conditions)
```

Observing _fig.3_, we can verify that when we have bad weather conditions like rain, fog and high winds, there's a higher frequency of fatal accidents.
We can also assume that when snowing, drivers have limitations on their driving conditions, and because of that the frequency of fatal accidents is almost null when compared to other conditions.

We also found interesting knowing the distribution of casualties. This graph could provide some perspective on the normal number of casualties per accident. So we generated the graph as you can see on _fig.4_.

```{r presuure, echo=FALSE}
# number of casualties by accident

ggplot(data, aes(x=factor(0), y=Number_of_Casualties)) +
  geom_boxplot() +
  ggtitle('Number of casualties (fig. 4)') +
  coord_flip()
```

We also considered important to analyze the severity of accidents in comparison to the type of road, as you can observe on _fig.5_.
 
```{r presuure2, echo=FALSE}

# x1st_class_road por accident_severity
accid <- read.csv("Accidents_2015.csv")
accid$Accident_Severity = Casualty_Severity[accid$Accident_Severity,]$label
accid$X1st_Road_Class = FirstRdClass[accid$X1st_Road_Class]
X1stClassRoadperTypeAccident <- select(accid, X1st_Road_Class, Accident_Severity)
X1stClassRoadperTypeAccident <- group_by(X1stClassRoadperTypeAccident, X1st_Road_Class, Accident_Severity)
X1stClassRoadperTypeAccident <- dplyr::count(X1stClassRoadperTypeAccident, X1st_Road_Class, Accident_Severity)
X1stClassRoadperTypeAccident <- mutate(X1stClassRoadperTypeAccident, fr=n/sum(n))
ggplot(X1stClassRoadperTypeAccident, aes(x = Accident_Severity, y = fr)) + geom_bar(stat="identity") + facet_wrap(~ X1st_Road_Class) + ggtitle("Accident Severity by each 1st Road Class (fig. 5)")

```

As we can check, the type road A, or primary road, is the type that shows more fatal, serious and slight accidents. This primary road belongs to a network that is fully connected, meaning you can reach any part from any other without leaving the network, making a preferable road for travelling for the drivers, so it is not a surprise that there is an elevated number of accidents when compared to the rest of road types.


Finally, we found necessary to analyse casualty severity in relation to the first point of impact. For this it was necessary to have extra data, on this case was necessary to have a dataset relative to the vehicles and casualties. This data was also made available by the United Kingdom Government at https://data.gov.uk/dataset/road-accidents-safety-data

In this case, we expected to find some relation between the fatal accidents and a specific impact, so we created the graph in _fig.6_.

```{r presuure3, echo=FALSE, warning=FALSE, message=FALSE}

#table with 1st point of impact and casualty severity for each accident
vehicles <- read.csv("Vehicles_2015.csv")
casualties <- read.csv("Casualties_2015.csv")
library(sqldf)
matchIndex <- sqldf("SELECT v.Accident_Index, c.Casualty_Severity, v.X1st_Point_of_Impact
                    FROM casualties as c, vehicles as v WHERE v.Accident_Index = c.Accident_Index")
matchIndex <- dplyr::count(matchIndex, X1st_Point_of_Impact, Casualty_Severity)
matchIndex <- subset(matchIndex, X1st_Point_of_Impact!=-1)
matchIndex$X1st_Point_of_Impact = FirstPointImpact[matchIndex$X1st_Point_of_Impact+1]
matchIndex$Casualty_Severity = Casualty_Severity[matchIndex$Casualty_Severity,]$label
ggplot(matchIndex, aes(y=n, x=Casualty_Severity)) + 
  geom_bar(stat="identity") +
  ggtitle('Severity of casualties by first point of impact (fig. 6)') + facet_wrap(~ X1st_Point_of_Impact)

```

It is quite interesting to look at this data, because it could lead to some type of improvement by the manufacturers of the vehicles for critical sections like to strengthen the bodywork of that section of the car.

We can observe that all the higher percentage of impacts that result in a fatality are from Front impacts. For the rest of the points of impacts, it is fair to say that the most common type of casualty is Slight.

##Clustering 

After exploring the data, we decided to select the attributes that we thought would give us the best association rules, but that task turned out to be impossible, since the dataset is so big that we found out that some attributes were good to find patterns in the conditions of the road, but not relevant to accidents where people are involved even eliminating some of columns that were not relevant to the pattern mining like the Longitude, Latitude, Police_Force, etc. 

In a way, we wanted to create groups of attributes to execute the Apriori algorithm, so we decided to use DBSCAN clustering algorithm in order to create these groups. Before we executed the DBSCAN algorithm we decide to remove the duplicated values of the data set since it wouldn't affect the result.

```{r data_apriori_unique}
data_apriori<-unique(data_apriori)
```

Recurring to a clustering algorithm would automate the selection of attributes to form groups and to discover and exclude the outlier points of the dataset. Since we decided to use DBSCAN algorithm, we needed to choose the minimal points (minPts) in the k-neighbourhood and the k-distance (eps). After some tries, we achieved a set of clusters that was reasonable with minPts=10 and eps=2:

```{r clusters}

  clusters <- dbscan(data_apriori, eps = 2, minPts = 10)
  table(clusters$cluster)
  plot(data_apriori,col=clusters$cluster)
  
```


With the clusters generated, we only needed to calculate the Apriori algorithm for each cluster. At the beginning, we didn't want to generate all the possible rules since many of those rules wouldn't be interesting so we created a Top 20 rules with highest lift value for each cluster with support=0.6, confidence=0.8 and lift>1.2.

```{r apriori_cluster_function, echo=FALSE}

apply_apriori_clusters <- function(data_set, cluster_set){
  n_clusters <- c(1:max(unique(cluster_set$cluster)))
  subsets <- list()
  for(i in n_clusters){
    tt <- data_set[cluster_set$cluster==i,]
    tt[] <- lapply(tt, factor)
    ap_ <- apriori(tt, parameter=list(supp=0.6, conf=0.8, target="rules", minlen=2, maxlen=1000), control=list(verbose=FALSE))
    ap_ <- ap_[!is.redundant(ap_, measure="confidence"),]
    ss <- head(sort(ap_, decreasing=TRUE, na.last=NA, by="lift", arem="aimp"), 20)
    ss <- subset(ss,lift>1.2)
    subsets[[i]] <- ss
  }
  subsets
}

subsets <- apply_apriori_clusters(data_apriori, clusters)

```